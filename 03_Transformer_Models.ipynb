{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COE379L Project 3: Transformer Model Fine-Tuning and Evaluation\n",
        "\n",
        "## Cross-Model Comparison for News Topic Classification\n",
        "\n",
        "This notebook covers:\n",
        "- RoBERTa-base model fine-tuning\n",
        "- Hugging Face Transformers integration\n",
        "- Model evaluation and performance metrics\n",
        "- Training time and inference latency measurement\n",
        "- Comparison with classical models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdamW\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Scikit-learn for metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    log_loss,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "print(\"\\nLibraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load AG News dataset from Hugging Face\n",
        "print(\"Loading AG News dataset...\")\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# Extract train and test splits\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "print(f\"Training samples: {len(train_data):,}\")\n",
        "print(f\"Test samples: {len(test_data):,}\")\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "num_labels = len(class_labels)\n",
        "print(f\"Number of classes: {num_labels}\")\n",
        "print(f\"Classes: {class_labels}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample data:\")\n",
        "print(train_data[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize RoBERTa Tokenizer and Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model name - using RoBERTa-base as specified in requirements\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "print(f\"Loading tokenizer and model: {model_name}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Test tokenization\n",
        "sample_text = train_data[0]['text']\n",
        "print(f\"\\nSample text: {sample_text[:100]}...\")\n",
        "encoded = tokenizer(sample_text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocess Dataset for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,  # RoBERTa max length\n",
        "        return_tensors=None  # Return as lists, not tensors\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing training data...\")\n",
        "train_tokenized = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']  # Remove original text column\n",
        ")\n",
        "\n",
        "print(\"Tokenizing test data...\")\n",
        "test_tokenized = test_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenization complete!\")\n",
        "print(f\"Training features: {train_tokenized.column_names}\")\n",
        "print(f\"Test features: {test_tokenized.column_names}\")\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Dataset formatted for PyTorch!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Metrics Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro\n",
        "    }\n",
        "\n",
        "print(\"Metrics function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure Training Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments with enhanced progress tracking\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./roberta-ag-news',\n",
        "    num_train_epochs=3,  # Start with 3 epochs, can adjust\n",
        "    per_device_train_batch_size=16,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,  # More frequent logging for better progress tracking\n",
        "    evaluation_strategy=\"steps\",  # Evaluate every N steps\n",
        "    eval_steps=500,  # Evaluate every 500 steps\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,  # Keep only 2 best models\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    logging_first_step=True,  # Log the first step\n",
        "    prediction_loss_only=False,  # Show more metrics during evaluation\n",
        ")\n",
        "\n",
        "# Calculate approximate total steps\n",
        "num_train_samples = len(train_tokenized)\n",
        "steps_per_epoch = num_train_samples // training_args.per_device_train_batch_size\n",
        "if torch.cuda.is_available():\n",
        "    steps_per_epoch = steps_per_epoch  # Single GPU\n",
        "else:\n",
        "    steps_per_epoch = steps_per_epoch  # CPU\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size (train): {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Batch size (eval): {training_args.per_device_eval_batch_size}\")\n",
        "print(f\"Mixed precision (FP16): {training_args.fp16}\")\n",
        "print(f\"Logging steps: {training_args.logging_steps}\")\n",
        "print(f\"Evaluation steps: {training_args.eval_steps}\")\n",
        "print(f\"Training samples: {num_train_samples:,}\")\n",
        "print(f\"Approximate steps per epoch: {steps_per_epoch:,}\")\n",
        "print(f\"Total training steps: ~{total_steps:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Trainer and Fine-Tune Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a custom callback for progress tracking\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    \"\"\"Custom callback to track and display training progress\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.epoch_start_time = None\n",
        "        self.last_log_time = None\n",
        "        \n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "        self.last_log_time = self.start_time\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TRAINING STARTED\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total epochs: {int(args.num_train_epochs)}\")\n",
        "        print(f\"Total steps: ~{state.max_steps:,}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Batch size: {args.per_device_train_batch_size}\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "        \n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        self.epoch_start_time = time.time()\n",
        "        current_epoch = int(state.epoch) + 1\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EPOCH {current_epoch}/{int(args.num_train_epochs)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            current_time = time.time()\n",
        "            elapsed_total = current_time - self.start_time\n",
        "            \n",
        "            # Display training progress\n",
        "            if 'loss' in logs and 'learning_rate' in logs:\n",
        "                step = state.global_step\n",
        "                loss = logs['loss']\n",
        "                lr = logs['learning_rate']\n",
        "                elapsed_since_last = current_time - self.last_log_time\n",
        "                \n",
        "                # Calculate progress percentage\n",
        "                progress_pct = (step / state.max_steps) * 100 if state.max_steps > 0 else 0\n",
        "                \n",
        "                print(f\"Step {step:6d}/{state.max_steps} ({progress_pct:5.1f}%) | \"\n",
        "                      f\"Loss: {loss:.4f} | LR: {lr:.2e} | \"\n",
        "                      f\"Time: {elapsed_total/60:6.1f}min | \"\n",
        "                      f\"Epoch: {state.epoch:.2f}/{args.num_train_epochs}\")\n",
        "                \n",
        "                self.last_log_time = current_time\n",
        "                \n",
        "            # Display evaluation results\n",
        "            if 'eval_loss' in logs:\n",
        "                print(f\"\\n{'─'*80}\")\n",
        "                print(f\"EVALUATION RESULTS (Step {state.global_step}):\")\n",
        "                print(f\"  Loss: {logs.get('eval_loss', 'N/A'):.4f}\")\n",
        "                print(f\"  Accuracy: {logs.get('eval_accuracy', 'N/A'):.4f}\")\n",
        "                print(f\"  F1-Macro: {logs.get('eval_f1_macro', 'N/A'):.4f}\")\n",
        "                eval_time = current_time - self.last_log_time\n",
        "                print(f\"  Eval time: {eval_time:.1f}s\")\n",
        "                print(f\"{'─'*80}\\n\")\n",
        "                self.last_log_time = current_time\n",
        "                \n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        epoch_time = time.time() - self.epoch_start_time\n",
        "        total_time = time.time() - self.start_time\n",
        "        current_epoch = int(state.epoch) + 1\n",
        "        \n",
        "        print(f\"\\n{'─'*80}\")\n",
        "        print(f\"Epoch {current_epoch} completed!\")\n",
        "        print(f\"  Epoch time: {epoch_time/60:.2f} minutes\")\n",
        "        print(f\"  Total time: {total_time/60:.2f} minutes\")\n",
        "        \n",
        "        if current_epoch < int(args.num_train_epochs):\n",
        "            remaining_epochs = int(args.num_train_epochs) - current_epoch\n",
        "            avg_epoch_time = total_time / current_epoch\n",
        "            estimated_remaining = (remaining_epochs * avg_epoch_time) / 60\n",
        "            print(f\"  Estimated time remaining: {estimated_remaining:.2f} minutes\")\n",
        "        print(f\"{'─'*80}\\n\")\n",
        "        \n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        total_time = time.time() - self.start_time\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TRAINING COMPLETED\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "        print(f\"Total steps completed: {state.global_step}\")\n",
        "        print(f\"Epochs completed: {state.epoch:.2f}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Create trainer with progress callback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=test_tokenized,  # Using test set for evaluation during training\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(early_stopping_patience=2),  # Stop if no improvement for 2 epochs\n",
        "        ProgressCallback()  # Custom progress tracker\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Trainer created with progress tracking!\")\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"Progress will be displayed below:\\n\")\n",
        "\n",
        "# Record training start time\n",
        "training_start_time = time.time()\n",
        "\n",
        "# Train the model (progress will be shown by the callback)\n",
        "trainer.train()\n",
        "\n",
        "# Record training end time\n",
        "training_time = time.time() - training_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Fine-tuning completed!\")\n",
        "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Model on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"  Macro F1-Score: {eval_results['eval_f1_macro']:.4f}\")\n",
        "\n",
        "# Get predictions for detailed metrics\n",
        "print(\"\\nGenerating predictions...\")\n",
        "predictions = trainer.predict(test_tokenized)\n",
        "\n",
        "# Extract predictions and labels\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_test = predictions.label_ids\n",
        "y_pred_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\nDetailed Metrics:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Macro F1-Score: {f1_macro:.4f}\")\n",
        "print(f\"  Log Loss: {log_loss_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Measure Inference Latency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure inference latency for 1000 samples\n",
        "print(\"Measuring inference latency...\")\n",
        "\n",
        "# Sample 1000 test examples\n",
        "num_samples = 1000\n",
        "sample_indices = np.random.choice(len(test_tokenized), num_samples, replace=False)\n",
        "sample_data = test_tokenized.select(sample_indices)\n",
        "\n",
        "# Warm-up\n",
        "_ = trainer.predict(sample_data.select(range(10)))\n",
        "\n",
        "# Measure inference time\n",
        "inference_start_time = time.time()\n",
        "_ = trainer.predict(sample_data)\n",
        "inference_time = time.time() - inference_start_time\n",
        "\n",
        "# Calculate latency per 1000 samples\n",
        "inference_latency_per_1k = inference_time\n",
        "\n",
        "print(f\"Inference time for {num_samples} samples: {inference_time:.4f} seconds\")\n",
        "print(f\"Inference latency per 1,000 samples: {inference_latency_per_1k:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results\n",
        "roberta_results = {\n",
        "    'Model': 'RoBERTa-base',\n",
        "    'Accuracy': accuracy,\n",
        "    'Macro F1-Score': f1_macro,\n",
        "    'Log Loss': log_loss_score,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Inference Latency per 1k (s)': inference_latency_per_1k\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame([roberta_results])\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ROBERTA MODEL - RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('data/roberta_results.csv', index=False)\n",
        "print(\"\\nResults saved to data/roberta_results.csv\")\n",
        "\n",
        "# Save model\n",
        "print(\"\\nSaving model...\")\n",
        "os.makedirs('data/roberta_ag_news_model', exist_ok=True)\n",
        "trainer.save_model('data/roberta_ag_news_model')\n",
        "tokenizer.save_pretrained('data/roberta_ag_news_model')\n",
        "print(\"Model saved to data/roberta_ag_news_model/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_labels, yticklabels=class_labels,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - RoBERTa-base', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/roberta_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Confusion matrix saved to data/roberta_confusion_matrix.png\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Compare with Classical Models\n",
        "\n",
        "Load classical models results and create comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load classical models results if available\n",
        "classical_results_path = 'data/classical_models_results.csv'\n",
        "\n",
        "if os.path.exists(classical_results_path):\n",
        "    classical_df = pd.read_csv(classical_results_path)\n",
        "    \n",
        "    # Combine results\n",
        "    all_results = pd.concat([classical_df, results_df], ignore_index=True)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"ALL MODELS - COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "    print(all_results.to_string(index=False))\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Save combined results\n",
        "    all_results.to_csv('data/all_models_results.csv', index=False)\n",
        "    print(\"\\nCombined results saved to data/all_models_results.csv\")\n",
        "    \n",
        "    # Visualization: F1-Score comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    models = all_results['Model'].values\n",
        "    f1_scores = all_results['Macro F1-Score'].values\n",
        "    training_times = all_results['Training Time (s)'].values\n",
        "    \n",
        "    # F1-Score comparison\n",
        "    axes[0].bar(models, f1_scores, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[0].set_title('Macro F1-Score Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Macro F1-Score', fontsize=12)\n",
        "    axes[0].set_ylim([0, 1])\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(f1_scores):\n",
        "        axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Training time comparison\n",
        "    axes[1].bar(models, training_times, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[1].set_title('Training Time Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
        "    axes[1].set_yscale('log')  # Log scale for better visualization\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(training_times):\n",
        "        axes[1].text(i, v * 1.2, f'{v:.1f}s', ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('data/all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Comparison visualization saved to data/all_models_comparison.png\")\n",
        "    \n",
        "else:\n",
        "    print(\"Classical models results not found. Run 02_Classical_Models.ipynb first.\")\n",
        "    print(\"RoBERTa results saved separately.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
