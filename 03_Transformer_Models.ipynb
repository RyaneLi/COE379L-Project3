{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COE379L Project 3: Transformer Model Fine-Tuning and Evaluation\n",
        "\n",
        "## Cross-Model Comparison for News Topic Classification\n",
        "\n",
        "This notebook covers:\n",
        "- RoBERTa-base model fine-tuning\n",
        "- Hugging Face Transformers integration\n",
        "- Model evaluation and performance metrics\n",
        "- Training time and inference latency measurement\n",
        "- Comparison with classical models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "\n",
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Scikit-learn for metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    log_loss,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "print(\"\\nLibraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LOADING AG NEWS DATASET\n",
            "================================================================================\n",
            "Loading from Hugging Face...\n",
            "Dataset loaded in 1.81 seconds\n",
            "\n",
            "Training samples: 120,000\n",
            "Test samples: 7,600\n",
            "Number of classes: 4\n",
            "Classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
            "\n",
            "Sample data:\n",
            "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
          ]
        }
      ],
      "source": [
        "# Load AG News dataset from Hugging Face\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING AG NEWS DATASET\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Loading from Hugging Face...\")\n",
        "load_start = time.time()\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "load_time = time.time() - load_start\n",
        "print(f\"Dataset loaded in {load_time:.2f} seconds\\n\")\n",
        "\n",
        "# Extract train and test splits\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "print(f\"Training samples: {len(train_data):,}\")\n",
        "print(f\"Test samples: {len(test_data):,}\")\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "num_labels = len(class_labels)\n",
        "print(f\"Number of classes: {num_labels}\")\n",
        "print(f\"Classes: {class_labels}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample data:\")\n",
        "print(train_data[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize RoBERTa Tokenizer and Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INITIALIZING ROBERTA MODEL\n",
            "================================================================================\n",
            "Loading tokenizer and model: roberta-base\n",
            "This may take a moment (downloading if not cached)...\n",
            "\n",
            "Loading tokenizer...\n",
            "  Tokenizer loaded in 1.73 seconds\n",
            "\n",
            "Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Model loaded in 20.81 seconds (0.35 minutes)\n",
            "\n",
            "Moving model to cpu...\n",
            "  Model moved to cpu\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "MODEL INFORMATION\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Model parameters: 124,648,708\n",
            "Trainable parameters: 124,648,708\n",
            "Device: cpu\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Sample text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\b...\n",
            "Tokenized shape: torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "# Model name - using RoBERTa-base as specified in requirements\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INITIALIZING ROBERTA MODEL\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Loading tokenizer and model: {model_name}\")\n",
        "print(\"This may take a moment (downloading if not cached)...\\n\")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer_start = time.time()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_time = time.time() - tokenizer_start\n",
        "print(f\"  Tokenizer loaded in {tokenizer_time:.2f} seconds\")\n",
        "\n",
        "# Load model for sequence classification\n",
        "print(\"\\nLoading model...\")\n",
        "model_start = time.time()\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "model_load_time = time.time() - model_start\n",
        "print(f\"  Model loaded in {model_load_time:.2f} seconds ({model_load_time/60:.2f} minutes)\")\n",
        "\n",
        "# Move model to device\n",
        "print(f\"\\nMoving model to {device}...\")\n",
        "model = model.to(device)\n",
        "print(f\"  Model moved to {device}\")\n",
        "\n",
        "print(f\"\\n{'─'*80}\")\n",
        "print(\"MODEL INFORMATION\")\n",
        "print(f\"{'─'*80}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Test tokenization\n",
        "sample_text = train_data[0]['text']\n",
        "print(f\"\\nSample text: {sample_text[:100]}...\")\n",
        "encoded = tokenizer(sample_text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocess Dataset for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TOKENIZING DATASETS\n",
            "================================================================================\n",
            "Training samples: 120,000\n",
            "Test samples: 7,600\n",
            "This may take a few minutes...\n",
            "\n",
            "Tokenizing training data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 120000/120000 [00:43<00:00, 2737.36 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Training set tokenized in 43.98 seconds (0.73 minutes)\n",
            "\n",
            "Tokenizing test data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 7600/7600 [00:03<00:00, 2001.29 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Test set tokenized in 3.91 seconds (0.07 minutes)\n",
            "\n",
            "Total tokenization time: 47.90 seconds (0.80 minutes)\n",
            "================================================================================\n",
            "\n",
            "Tokenization complete!\n",
            "Training features: ['label', 'input_ids', 'attention_mask']\n",
            "Test features: ['label', 'input_ids', 'attention_mask']\n",
            "Dataset formatted for PyTorch!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,  # RoBERTa max length\n",
        "        return_tensors=None  # Return as lists, not tensors\n",
        "    )\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TOKENIZING DATASETS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Training samples: {len(train_data):,}\")\n",
        "print(f\"Test samples: {len(test_data):,}\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "print(\"Tokenizing training data...\")\n",
        "tokenize_start = time.time()\n",
        "train_tokenized = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']  # Remove original text column\n",
        ")\n",
        "train_tokenize_time = time.time() - tokenize_start\n",
        "print(f\"  Training set tokenized in {train_tokenize_time:.2f} seconds ({train_tokenize_time/60:.2f} minutes)\")\n",
        "\n",
        "print(\"\\nTokenizing test data...\")\n",
        "test_tokenize_start = time.time()\n",
        "test_tokenized = test_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")\n",
        "test_tokenize_time = time.time() - test_tokenize_start\n",
        "print(f\"  Test set tokenized in {test_tokenize_time:.2f} seconds ({test_tokenize_time/60:.2f} minutes)\")\n",
        "\n",
        "total_tokenize_time = time.time() - tokenize_start\n",
        "print(f\"\\nTotal tokenization time: {total_tokenize_time:.2f} seconds ({total_tokenize_time/60:.2f} minutes)\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(f\"Tokenization complete!\")\n",
        "print(f\"Training features: {train_tokenized.column_names}\")\n",
        "print(f\"Test features: {test_tokenized.column_names}\")\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Dataset formatted for PyTorch!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Metrics Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics function defined!\n"
          ]
        }
      ],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro\n",
        "    }\n",
        "\n",
        "print(\"Metrics function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure Training Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.STEPS\n- Save strategy: SaveStrategy.EPOCH",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Training arguments with enhanced progress tracking\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./roberta-ag-news\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start with 3 epochs, can adjust\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on GPU memory\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# More frequent logging for better progress tracking\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Evaluate every N steps (changed from evaluation_strategy in newer transformers)\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Evaluate every 500 steps\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf1_macro\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep only 2 best models\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use mixed precision if GPU available\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader_num_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable wandb/tensorboard\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_first_step\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Log the first step\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Show more metrics during evaluation\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Calculate approximate total steps\u001b[39;00m\n\u001b[32m     26\u001b[39m num_train_samples = \u001b[38;5;28mlen\u001b[39m(train_tokenized)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<string>:135\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Github/COE379L-Project3/venv/lib/python3.13/site-packages/transformers/training_args.py:1689\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_strategy != SaveStrategy.BEST:\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy != \u001b[38;5;28mself\u001b[39m.save_strategy:\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1690\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Evaluation \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1691\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.eval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.save_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1692\u001b[39m         )\n\u001b[32m   1693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy == IntervalStrategy.STEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps % \u001b[38;5;28mself\u001b[39m.eval_steps != \u001b[32m0\u001b[39m:\n\u001b[32m   1694\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_steps < \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps < \u001b[32m1\u001b[39m:\n",
            "\u001b[31mValueError\u001b[39m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.STEPS\n- Save strategy: SaveStrategy.EPOCH"
          ]
        }
      ],
      "source": [
        "# Training arguments with enhanced progress tracking\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./roberta-ag-news',\n",
        "    num_train_epochs=3,  # Start with 3 epochs, can adjust\n",
        "    per_device_train_batch_size=16,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,  # More frequent logging for better progress tracking\n",
        "    eval_strategy=\"steps\",  # Evaluate every N steps (changed from evaluation_strategy in newer transformers)\n",
        "    eval_steps=500,  # Evaluate every 500 steps\n",
        "    save_strategy=\"steps\",  # Must match eval_strategy when load_best_model_at_end=True\n",
        "    save_steps=500,  # Save every 500 steps (matching eval_steps)\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,  # Keep only 2 best models\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    logging_first_step=True,  # Log the first step\n",
        "    prediction_loss_only=False,  # Show more metrics during evaluation\n",
        ")\n",
        "\n",
        "# Calculate approximate total steps\n",
        "num_train_samples = len(train_tokenized)\n",
        "steps_per_epoch = num_train_samples // training_args.per_device_train_batch_size\n",
        "if torch.cuda.is_available():\n",
        "    steps_per_epoch = steps_per_epoch  # Single GPU\n",
        "else:\n",
        "    steps_per_epoch = steps_per_epoch  # CPU\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size (train): {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Batch size (eval): {training_args.per_device_eval_batch_size}\")\n",
        "print(f\"Mixed precision (FP16): {training_args.fp16}\")\n",
        "print(f\"Logging steps: {training_args.logging_steps}\")\n",
        "print(f\"Evaluation steps: {training_args.eval_steps}\")\n",
        "print(f\"Training samples: {num_train_samples:,}\")\n",
        "print(f\"Approximate steps per epoch: {steps_per_epoch:,}\")\n",
        "print(f\"Total training steps: ~{total_steps:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Trainer and Fine-Tune Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a custom callback for progress tracking\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    \"\"\"Custom callback to track and display training progress\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.epoch_start_time = None\n",
        "        self.last_log_time = None\n",
        "        \n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "        self.last_log_time = self.start_time\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TRAINING STARTED\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total epochs: {int(args.num_train_epochs)}\")\n",
        "        print(f\"Total steps: ~{state.max_steps:,}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Batch size: {args.per_device_train_batch_size}\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "        \n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        self.epoch_start_time = time.time()\n",
        "        current_epoch = int(state.epoch) + 1\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EPOCH {current_epoch}/{int(args.num_train_epochs)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            current_time = time.time()\n",
        "            elapsed_total = current_time - self.start_time\n",
        "            \n",
        "            # Display training progress\n",
        "            if 'loss' in logs and 'learning_rate' in logs:\n",
        "                step = state.global_step\n",
        "                loss = logs['loss']\n",
        "                lr = logs['learning_rate']\n",
        "                elapsed_since_last = current_time - self.last_log_time\n",
        "                \n",
        "                # Calculate progress percentage\n",
        "                progress_pct = (step / state.max_steps) * 100 if state.max_steps > 0 else 0\n",
        "                \n",
        "                print(f\"Step {step:6d}/{state.max_steps} ({progress_pct:5.1f}%) | \"\n",
        "                      f\"Loss: {loss:.4f} | LR: {lr:.2e} | \"\n",
        "                      f\"Time: {elapsed_total/60:6.1f}min | \"\n",
        "                      f\"Epoch: {state.epoch:.2f}/{args.num_train_epochs}\")\n",
        "                \n",
        "                self.last_log_time = current_time\n",
        "                \n",
        "            # Display evaluation results\n",
        "            if 'eval_loss' in logs:\n",
        "                print(f\"\\n{'─'*80}\")\n",
        "                print(f\"EVALUATION RESULTS (Step {state.global_step}):\")\n",
        "                print(f\"  Loss: {logs.get('eval_loss', 'N/A'):.4f}\")\n",
        "                print(f\"  Accuracy: {logs.get('eval_accuracy', 'N/A'):.4f}\")\n",
        "                print(f\"  F1-Macro: {logs.get('eval_f1_macro', 'N/A'):.4f}\")\n",
        "                eval_time = current_time - self.last_log_time\n",
        "                print(f\"  Eval time: {eval_time:.1f}s\")\n",
        "                print(f\"{'─'*80}\\n\")\n",
        "                self.last_log_time = current_time\n",
        "                \n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        epoch_time = time.time() - self.epoch_start_time\n",
        "        total_time = time.time() - self.start_time\n",
        "        current_epoch = int(state.epoch) + 1\n",
        "        \n",
        "        print(f\"\\n{'─'*80}\")\n",
        "        print(f\"Epoch {current_epoch} completed!\")\n",
        "        print(f\"  Epoch time: {epoch_time/60:.2f} minutes\")\n",
        "        print(f\"  Total time: {total_time/60:.2f} minutes\")\n",
        "        \n",
        "        if current_epoch < int(args.num_train_epochs):\n",
        "            remaining_epochs = int(args.num_train_epochs) - current_epoch\n",
        "            avg_epoch_time = total_time / current_epoch\n",
        "            estimated_remaining = (remaining_epochs * avg_epoch_time) / 60\n",
        "            print(f\"  Estimated time remaining: {estimated_remaining:.2f} minutes\")\n",
        "        print(f\"{'─'*80}\\n\")\n",
        "        \n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        total_time = time.time() - self.start_time\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TRAINING COMPLETED\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "        print(f\"Total steps completed: {state.global_step}\")\n",
        "        print(f\"Epochs completed: {state.epoch:.2f}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Create trainer with progress callback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=test_tokenized,  # Using test set for evaluation during training\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(early_stopping_patience=2),  # Stop if no improvement for 2 epochs\n",
        "        ProgressCallback()  # Custom progress tracker\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Trainer created with progress tracking!\")\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"Progress will be displayed below:\\n\")\n",
        "\n",
        "# Record training start time\n",
        "training_start_time = time.time()\n",
        "\n",
        "# Train the model (progress will be shown by the callback)\n",
        "trainer.train()\n",
        "\n",
        "# Record training end time\n",
        "training_time = time.time() - training_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Fine-tuning completed!\")\n",
        "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Model on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set with progress tracking\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATING MODEL ON TEST SET\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Test samples: {len(test_tokenized):,}\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Step 1: Running evaluation...\")\n",
        "eval_start = time.time()\n",
        "eval_results = trainer.evaluate()\n",
        "eval_time = time.time() - eval_start\n",
        "\n",
        "print(f\"\\n{'─'*80}\")\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"  Macro F1-Score: {eval_results['eval_f1_macro']:.4f}\")\n",
        "print(f\"  Evaluation time: {eval_time:.2f} seconds ({eval_time/60:.2f} minutes)\")\n",
        "print(f\"{'─'*80}\\n\")\n",
        "\n",
        "# Get predictions for detailed metrics\n",
        "print(\"Step 2: Generating predictions on full test set...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "pred_start = time.time()\n",
        "predictions = trainer.predict(test_tokenized)\n",
        "pred_time = time.time() - pred_start\n",
        "\n",
        "print(f\"Predictions completed in {pred_time:.2f} seconds ({pred_time/60:.2f} minutes)\")\n",
        "\n",
        "# Extract predictions and labels\n",
        "print(\"\\nStep 3: Processing predictions and calculating metrics...\")\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_test = predictions.label_ids\n",
        "y_pred_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"DETAILED METRICS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Macro F1-Score: {f1_macro:.4f}\")\n",
        "print(f\"  Log Loss: {log_loss_score:.4f}\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Measure Inference Latency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure inference latency for 1000 samples with progress tracking\n",
        "print(\"=\" * 80)\n",
        "print(\"MEASURING INFERENCE LATENCY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Sample 1000 test examples\n",
        "num_samples = 1000\n",
        "print(f\"Sampling {num_samples} test examples...\")\n",
        "sample_indices = np.random.choice(len(test_tokenized), num_samples, replace=False)\n",
        "sample_data = test_tokenized.select(sample_indices)\n",
        "\n",
        "# Warm-up\n",
        "print(\"Warming up model (running 10 samples)...\")\n",
        "warmup_start = time.time()\n",
        "_ = trainer.predict(sample_data.select(range(10)))\n",
        "warmup_time = time.time() - warmup_start\n",
        "print(f\"Warm-up completed in {warmup_time:.2f} seconds\\n\")\n",
        "\n",
        "# Measure inference time\n",
        "print(f\"Measuring inference time for {num_samples} samples...\")\n",
        "print(\"This may take a minute or two...\")\n",
        "inference_start_time = time.time()\n",
        "_ = trainer.predict(sample_data)\n",
        "inference_time = time.time() - inference_start_time\n",
        "\n",
        "# Calculate latency per 1000 samples\n",
        "inference_latency_per_1k = inference_time\n",
        "\n",
        "print(f\"\\n{'─'*80}\")\n",
        "print(\"INFERENCE LATENCY RESULTS\")\n",
        "print(f\"{'─'*80}\")\n",
        "print(f\"Inference time for {num_samples} samples: {inference_time:.4f} seconds\")\n",
        "print(f\"Inference latency per 1,000 samples: {inference_latency_per_1k:.4f} seconds\")\n",
        "print(f\"Average latency per sample: {inference_latency_per_1k/1000:.4f} seconds\")\n",
        "print(f\"{'─'*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results\n",
        "roberta_results = {\n",
        "    'Model': 'RoBERTa-base',\n",
        "    'Accuracy': accuracy,\n",
        "    'Macro F1-Score': f1_macro,\n",
        "    'Log Loss': log_loss_score,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Inference Latency per 1k (s)': inference_latency_per_1k\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame([roberta_results])\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ROBERTA MODEL - RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('data/roberta_results.csv', index=False)\n",
        "print(\"\\nResults saved to data/roberta_results.csv\")\n",
        "\n",
        "# Save model with progress tracking\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Saving model and tokenizer...\")\n",
        "print(\"This may take a minute...\")\n",
        "\n",
        "save_start = time.time()\n",
        "os.makedirs('data/roberta_ag_news_model', exist_ok=True)\n",
        "trainer.save_model('data/roberta_ag_news_model')\n",
        "print(\"  ✓ Model saved\")\n",
        "tokenizer.save_pretrained('data/roberta_ag_news_model')\n",
        "print(\"  ✓ Tokenizer saved\")\n",
        "save_time = time.time() - save_start\n",
        "\n",
        "print(f\"\\nModel saved to data/roberta_ag_news_model/\")\n",
        "print(f\"Save time: {save_time:.2f} seconds ({save_time/60:.2f} minutes)\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_labels, yticklabels=class_labels,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - RoBERTa-base', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/roberta_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Confusion matrix saved to data/roberta_confusion_matrix.png\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Compare with Classical Models\n",
        "\n",
        "Load classical models results and create comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load classical models results if available\n",
        "classical_results_path = 'data/classical_models_results.csv'\n",
        "\n",
        "if os.path.exists(classical_results_path):\n",
        "    classical_df = pd.read_csv(classical_results_path)\n",
        "    \n",
        "    # Combine results\n",
        "    all_results = pd.concat([classical_df, results_df], ignore_index=True)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"ALL MODELS - COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "    print(all_results.to_string(index=False))\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Save combined results\n",
        "    all_results.to_csv('data/all_models_results.csv', index=False)\n",
        "    print(\"\\nCombined results saved to data/all_models_results.csv\")\n",
        "    \n",
        "    # Visualization: F1-Score comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    models = all_results['Model'].values\n",
        "    f1_scores = all_results['Macro F1-Score'].values\n",
        "    training_times = all_results['Training Time (s)'].values\n",
        "    \n",
        "    # F1-Score comparison\n",
        "    axes[0].bar(models, f1_scores, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[0].set_title('Macro F1-Score Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Macro F1-Score', fontsize=12)\n",
        "    axes[0].set_ylim([0, 1])\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(f1_scores):\n",
        "        axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Training time comparison\n",
        "    axes[1].bar(models, training_times, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[1].set_title('Training Time Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
        "    axes[1].set_yscale('log')  # Log scale for better visualization\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(training_times):\n",
        "        axes[1].text(i, v * 1.2, f'{v:.1f}s', ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('data/all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Comparison visualization saved to data/all_models_comparison.png\")\n",
        "    \n",
        "else:\n",
        "    print(\"Classical models results not found. Run 02_Classical_Models.ipynb first.\")\n",
        "    print(\"RoBERTa results saved separately.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
