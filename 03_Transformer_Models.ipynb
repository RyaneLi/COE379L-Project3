{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COE379L Project 3: Transformer Model Fine-Tuning and Evaluation\n",
        "\n",
        "## Cross-Model Comparison for News Topic Classification\n",
        "\n",
        "This notebook covers:\n",
        "- RoBERTa-base model fine-tuning\n",
        "- Hugging Face Transformers integration\n",
        "- Model evaluation and performance metrics\n",
        "- Training time and inference latency measurement\n",
        "- Comparison with classical models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Scikit-learn for metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    log_loss,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "print(\"\\nLibraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load AG News dataset from Hugging Face\n",
        "print(\"Loading AG News dataset...\")\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# Extract train and test splits\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "print(f\"Training samples: {len(train_data):,}\")\n",
        "print(f\"Test samples: {len(test_data):,}\")\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "num_labels = len(class_labels)\n",
        "print(f\"Number of classes: {num_labels}\")\n",
        "print(f\"Classes: {class_labels}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample data:\")\n",
        "print(train_data[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize RoBERTa Tokenizer and Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model name - using RoBERTa-base as specified in requirements\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "print(f\"Loading tokenizer and model: {model_name}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Test tokenization\n",
        "sample_text = train_data[0]['text']\n",
        "print(f\"\\nSample text: {sample_text[:100]}...\")\n",
        "encoded = tokenizer(sample_text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocess Dataset for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,  # RoBERTa max length\n",
        "        return_tensors=None  # Return as lists, not tensors\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing training data...\")\n",
        "train_tokenized = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']  # Remove original text column\n",
        ")\n",
        "\n",
        "print(\"Tokenizing test data...\")\n",
        "test_tokenized = test_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenization complete!\")\n",
        "print(f\"Training features: {train_tokenized.column_names}\")\n",
        "print(f\"Test features: {test_tokenized.column_names}\")\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Dataset formatted for PyTorch!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Metrics Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro\n",
        "    }\n",
        "\n",
        "print(\"Metrics function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure Training Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./roberta-ag-news',\n",
        "    num_train_epochs=3,  # Start with 3 epochs, can adjust\n",
        "    per_device_train_batch_size=16,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at end of each epoch\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,  # Keep only 2 best models\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size (train): {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Batch size (eval): {training_args.per_device_eval_batch_size}\")\n",
        "print(f\"Mixed precision (FP16): {training_args.fp16}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Trainer and Fine-Tune Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=test_tokenized,  # Using test set for evaluation during training\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stop if no improvement for 2 epochs\n",
        ")\n",
        "\n",
        "print(\"Trainer created!\")\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Record training start time\n",
        "training_start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Record training end time\n",
        "training_time = time.time() - training_start_time\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"Fine-tuning completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Model on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"  Macro F1-Score: {eval_results['eval_f1_macro']:.4f}\")\n",
        "\n",
        "# Get predictions for detailed metrics\n",
        "print(\"\\nGenerating predictions...\")\n",
        "predictions = trainer.predict(test_tokenized)\n",
        "\n",
        "# Extract predictions and labels\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_test = predictions.label_ids\n",
        "y_pred_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\nDetailed Metrics:\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Macro F1-Score: {f1_macro:.4f}\")\n",
        "print(f\"  Log Loss: {log_loss_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Measure Inference Latency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure inference latency for 1000 samples\n",
        "print(\"Measuring inference latency...\")\n",
        "\n",
        "# Sample 1000 test examples\n",
        "num_samples = 1000\n",
        "sample_indices = np.random.choice(len(test_tokenized), num_samples, replace=False)\n",
        "sample_data = test_tokenized.select(sample_indices)\n",
        "\n",
        "# Warm-up\n",
        "_ = trainer.predict(sample_data.select(range(10)))\n",
        "\n",
        "# Measure inference time\n",
        "inference_start_time = time.time()\n",
        "_ = trainer.predict(sample_data)\n",
        "inference_time = time.time() - inference_start_time\n",
        "\n",
        "# Calculate latency per 1000 samples\n",
        "inference_latency_per_1k = inference_time\n",
        "\n",
        "print(f\"Inference time for {num_samples} samples: {inference_time:.4f} seconds\")\n",
        "print(f\"Inference latency per 1,000 samples: {inference_latency_per_1k:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results\n",
        "roberta_results = {\n",
        "    'Model': 'RoBERTa-base',\n",
        "    'Accuracy': accuracy,\n",
        "    'Macro F1-Score': f1_macro,\n",
        "    'Log Loss': log_loss_score,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Inference Latency per 1k (s)': inference_latency_per_1k\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame([roberta_results])\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ROBERTA MODEL - RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('data/roberta_results.csv', index=False)\n",
        "print(\"\\nResults saved to data/roberta_results.csv\")\n",
        "\n",
        "# Save model\n",
        "print(\"\\nSaving model...\")\n",
        "trainer.save_model('data/roberta_ag_news_model')\n",
        "tokenizer.save_pretrained('data/roberta_ag_news_model')\n",
        "print(\"Model saved to data/roberta_ag_news_model/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_labels, yticklabels=class_labels,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - RoBERTa-base', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/roberta_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Confusion matrix saved to data/roberta_confusion_matrix.png\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Compare with Classical Models\n",
        "\n",
        "Load classical models results and create comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load classical models results if available\n",
        "classical_results_path = 'data/classical_models_results.csv'\n",
        "\n",
        "if os.path.exists(classical_results_path):\n",
        "    classical_df = pd.read_csv(classical_results_path)\n",
        "    \n",
        "    # Combine results\n",
        "    all_results = pd.concat([classical_df, results_df], ignore_index=True)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"ALL MODELS - COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "    print(all_results.to_string(index=False))\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Save combined results\n",
        "    all_results.to_csv('data/all_models_results.csv', index=False)\n",
        "    print(\"\\nCombined results saved to data/all_models_results.csv\")\n",
        "    \n",
        "    # Visualization: F1-Score comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    models = all_results['Model'].values\n",
        "    f1_scores = all_results['Macro F1-Score'].values\n",
        "    training_times = all_results['Training Time (s)'].values\n",
        "    \n",
        "    # F1-Score comparison\n",
        "    axes[0].bar(models, f1_scores, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[0].set_title('Macro F1-Score Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Macro F1-Score', fontsize=12)\n",
        "    axes[0].set_ylim([0, 1])\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(f1_scores):\n",
        "        axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Training time comparison\n",
        "    axes[1].bar(models, training_times, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[1].set_title('Training Time Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
        "    axes[1].set_yscale('log')  # Log scale for better visualization\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(training_times):\n",
        "        axes[1].text(i, v * 1.2, f'{v:.1f}s', ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('data/all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Comparison visualization saved to data/all_models_comparison.png\")\n",
        "    \n",
        "else:\n",
        "    print(\"Classical models results not found. Run 02_Classical_Models.ipynb first.\")\n",
        "    print(\"RoBERTa results saved separately.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
