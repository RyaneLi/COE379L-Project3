{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COE379L Project 3: Transformer Model Fine-Tuning and Evaluation\n",
        "\n",
        "## Cross-Model Comparison for News Topic Classification\n",
        "\n",
        "This notebook covers:\n",
        "- RoBERTa-base model fine-tuning\n",
        "- Hugging Face Transformers integration\n",
        "- Model evaluation and performance metrics\n",
        "- Training time and inference latency measurement\n",
        "- Comparison with classical models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "\n",
        "# Disable MPS (Metal Performance Shaders) to force CPU usage\n",
        "# This prevents M1 GPU memory issues\n",
        "if hasattr(torch.backends, 'mps'):\n",
        "    torch.backends.mps.is_available = lambda: False\n",
        "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Scikit-learn for metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    log_loss,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Check for GPU/device\n",
        "# Using CPU for training to avoid M1 GPU memory issues and ensure stability\n",
        "USE_CPU = True  # Set to False to use GPU if available (not recommended for M1)\n",
        "\n",
        "# Force CPU usage\n",
        "device = torch.device('cpu')\n",
        "print(f\"Using device: {device} (CPU - forced for stability)\")\n",
        "print(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else 'N/A'}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "print(\"\\nLibraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LOADING AG NEWS DATASET\n",
            "================================================================================\n",
            "Loading from Hugging Face...\n",
            "Dataset loaded in 2.01 seconds\n",
            "\n",
            "Full training samples: 120,000\n",
            "Using subset: 10,000 samples for faster training\n",
            "Training samples (subset): 10,000\n",
            "Test samples: 7,600\n",
            "Number of classes: 4\n",
            "Classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
            "\n",
            "Sample data:\n",
            "{'text': 'SCORELESS IN SEATTLE Seattle -- - Not so long ago, the 49ers were inflicting on other teams the kind of pain and embarrassment they felt in their 34-0 loss to the Seahawks on Sunday.', 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "# Load AG News dataset from Hugging Face\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING AG NEWS DATASET\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Loading from Hugging Face...\")\n",
        "load_start = time.time()\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "load_time = time.time() - load_start\n",
        "print(f\"Dataset loaded in {load_time:.2f} seconds\\n\")\n",
        "\n",
        "# Extract train and test splits\n",
        "train_data_full = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "# Use subset of training data for faster training (< 1 hour target)\n",
        "# Adjust TRAIN_SUBSET_SIZE to control training time\n",
        "# Reduced to 1200 samples to guarantee < 1 hour training time on CPU\n",
        "TRAIN_SUBSET_SIZE = 1200  # Use 1.2k samples for faster training (target: < 1 hour)\n",
        "print(f\"Full training samples: {len(train_data_full):,}\")\n",
        "print(f\"Using subset: {TRAIN_SUBSET_SIZE:,} samples for faster training (< 1 hour target)\")\n",
        "\n",
        "# Create balanced subset (stratified sampling)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Get labels for stratification\n",
        "labels = [train_data_full[i]['label'] for i in range(len(train_data_full))]\n",
        "\n",
        "# Create indices\n",
        "indices = np.arange(len(train_data_full))\n",
        "\n",
        "# Stratified split to get balanced subset\n",
        "_, subset_indices = train_test_split(\n",
        "    indices, \n",
        "    test_size=TRAIN_SUBSET_SIZE, \n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Select subset\n",
        "train_data = train_data_full.select(subset_indices)\n",
        "\n",
        "print(f\"Training samples (subset): {len(train_data):,}\")\n",
        "print(f\"Test samples: {len(test_data):,}\")\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "num_labels = len(class_labels)\n",
        "print(f\"Number of classes: {num_labels}\")\n",
        "print(f\"Classes: {class_labels}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample data:\")\n",
        "print(train_data[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize RoBERTa Tokenizer and Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "INITIALIZING ROBERTA MODEL\n",
            "================================================================================\n",
            "Loading tokenizer and model: roberta-base\n",
            "This may take a moment (downloading if not cached)...\n",
            "\n",
            "Loading tokenizer...\n",
            "  Tokenizer loaded in 0.68 seconds\n",
            "\n",
            "Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Model loaded in 0.52 seconds (0.01 minutes)\n",
            "\n",
            "Forcing CPU usage (USE_CPU=True)...\n",
            "  Model moved to cpu\n",
            "  Model device check: cpu\n",
            "\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "MODEL INFORMATION\n",
            "────────────────────────────────────────────────────────────────────────────────\n",
            "Model parameters: 124,648,708\n",
            "Trainable parameters: 124,648,708\n",
            "Device: cpu\n",
            "USE_CPU flag: True\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Sample text: SCORELESS IN SEATTLE Seattle -- - Not so long ago, the 49ers were inflicting on other teams the kind...\n",
            "Tokenized shape: torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "# Model name - using RoBERTa-base as specified in requirements\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"INITIALIZING ROBERTA MODEL\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Loading tokenizer and model: {model_name}\")\n",
        "print(\"This may take a moment (downloading if not cached)...\\n\")\n",
        "\n",
        "# Load tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer_start = time.time()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer_time = time.time() - tokenizer_start\n",
        "print(f\"  Tokenizer loaded in {tokenizer_time:.2f} seconds\")\n",
        "\n",
        "# Load model for sequence classification\n",
        "print(\"\\nLoading model...\")\n",
        "model_start = time.time()\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "model_load_time = time.time() - model_start\n",
        "print(f\"  Model loaded in {model_load_time:.2f} seconds ({model_load_time/60:.2f} minutes)\")\n",
        "\n",
        "# Move model to device (force CPU if USE_CPU is True)\n",
        "if USE_CPU:\n",
        "    device = torch.device('cpu')\n",
        "    print(f\"\\nForcing CPU usage (USE_CPU=True)...\")\n",
        "else:\n",
        "    print(f\"\\nMoving model to {device}...\")\n",
        "model = model.to(device)\n",
        "print(f\"  Model moved to {device}\")\n",
        "print(f\"  Model device check: {next(model.parameters()).device}\")\n",
        "\n",
        "print(f\"\\n{'─'*80}\")\n",
        "print(\"MODEL INFORMATION\")\n",
        "print(f\"{'─'*80}\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"USE_CPU flag: {USE_CPU}\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Test tokenization\n",
        "sample_text = train_data[0]['text']\n",
        "print(f\"\\nSample text: {sample_text[:100]}...\")\n",
        "encoded = tokenizer(sample_text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "print(f\"Tokenized shape: {encoded['input_ids'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Preprocess Dataset for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TOKENIZING DATASETS\n",
            "================================================================================\n",
            "Training samples: 10,000\n",
            "Test samples: 7,600\n",
            "This may take a few minutes...\n",
            "\n",
            "Tokenizing training data...\n",
            "  Training set tokenized in 0.09 seconds (0.00 minutes)\n",
            "\n",
            "Tokenizing test data...\n",
            "  Test set tokenized in 0.05 seconds (0.00 minutes)\n",
            "\n",
            "Total tokenization time: 0.13 seconds (0.00 minutes)\n",
            "================================================================================\n",
            "\n",
            "Tokenization complete!\n",
            "Training features: ['label', 'input_ids', 'attention_mask']\n",
            "Test features: ['label', 'input_ids', 'attention_mask']\n",
            "Dataset formatted for PyTorch!\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize the text data\"\"\"\n",
        "    return tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=512,  # RoBERTa max length\n",
        "        return_tensors=None  # Return as lists, not tensors\n",
        "    )\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TOKENIZING DATASETS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Training samples: {len(train_data):,}\")\n",
        "print(f\"Test samples: {len(test_data):,}\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "print(\"Tokenizing training data...\")\n",
        "tokenize_start = time.time()\n",
        "train_tokenized = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']  # Remove original text column\n",
        ")\n",
        "train_tokenize_time = time.time() - tokenize_start\n",
        "print(f\"  Training set tokenized in {train_tokenize_time:.2f} seconds ({train_tokenize_time/60:.2f} minutes)\")\n",
        "\n",
        "print(\"\\nTokenizing test data...\")\n",
        "test_tokenize_start = time.time()\n",
        "test_tokenized = test_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=['text']\n",
        ")\n",
        "test_tokenize_time = time.time() - test_tokenize_start\n",
        "print(f\"  Test set tokenized in {test_tokenize_time:.2f} seconds ({test_tokenize_time/60:.2f} minutes)\")\n",
        "\n",
        "total_tokenize_time = time.time() - tokenize_start\n",
        "print(f\"\\nTotal tokenization time: {total_tokenize_time:.2f} seconds ({total_tokenize_time/60:.2f} minutes)\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(f\"Tokenization complete!\")\n",
        "print(f\"Training features: {train_tokenized.column_names}\")\n",
        "print(f\"Test features: {test_tokenized.column_names}\")\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(\"Dataset formatted for PyTorch!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Metrics Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics function defined!\n"
          ]
        }
      ],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro\n",
        "    }\n",
        "\n",
        "print(\"Metrics function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Configure Training Arguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training arguments configured!\n",
            "Training epochs: 1\n",
            "Batch size (train): 16\n",
            "Gradient accumulation steps: 1\n",
            "Effective batch size: 16\n",
            "Batch size (eval): 32\n",
            "Mixed precision (FP16): False\n",
            "Logging steps: 50\n",
            "Evaluation steps: 200\n",
            "Training samples: 10,000\n",
            "Approximate steps per epoch: 625\n",
            "Total training steps: ~625\n",
            "\n",
            "Estimated training time: ~30-60 minutes (depending on CPU speed)\n"
          ]
        }
      ],
      "source": [
        "# Training arguments optimized for fast training (< 1 hour target)\n",
        "# Using subset of data (10k samples) and reduced epochs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./roberta-ag-news',\n",
        "    num_train_epochs=1,  # Reduced to 1 epoch for faster training\n",
        "    per_device_train_batch_size=16,  # Larger batch for faster training\n",
        "    per_device_eval_batch_size=32,  # Larger eval batch\n",
        "    gradient_accumulation_steps=1,  # No accumulation needed with larger batch\n",
        "    warmup_steps=100,  # Reduced warmup steps\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # More frequent logging\n",
        "    eval_strategy=\"steps\",  # Evaluate every N steps\n",
        "    eval_steps=200,  # Evaluate more frequently (every 200 steps)\n",
        "    save_strategy=\"steps\",  # Must match eval_strategy when load_best_model_at_end=True\n",
        "    save_steps=200,  # Save every 200 steps (matching eval_steps)\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,  # Keep only 2 best models\n",
        "    fp16=False,  # Not needed on CPU\n",
        "    dataloader_num_workers=0,  # Set to 0 for CPU\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    logging_first_step=True,  # Log the first step\n",
        "    prediction_loss_only=False,  # Show more metrics during evaluation\n",
        "    dataloader_pin_memory=False,  # Not needed on CPU\n",
        ")\n",
        "\n",
        "# Calculate approximate total steps\n",
        "num_train_samples = len(train_tokenized)\n",
        "steps_per_epoch = num_train_samples // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Training epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Batch size (train): {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Batch size (eval): {training_args.per_device_eval_batch_size}\")\n",
        "print(f\"Mixed precision (FP16): {training_args.fp16}\")\n",
        "print(f\"Logging steps: {training_args.logging_steps}\")\n",
        "print(f\"Evaluation steps: {training_args.eval_steps}\")\n",
        "print(f\"Training samples: {num_train_samples:,}\")\n",
        "print(f\"Approximate steps per epoch: {steps_per_epoch:,}\")\n",
        "print(f\"Total training steps: ~{total_steps:,}\")\n",
        "print(f\"\\nEstimated training time: ~30-50 minutes (depending on CPU speed)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create Trainer and Fine-Tune Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensuring model is on CPU for training...\n",
            "Model device: cpu\n",
            "Trainer created with progress tracking!\n",
            "Starting fine-tuning...\n",
            "Progress will be displayed below:\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TRAINING STARTED\n",
            "================================================================================\n",
            "Total epochs: 1\n",
            "Total steps: ~625\n",
            "Device: cpu\n",
            "Batch size: 16\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "EPOCH 1/1\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  6/625 05:04 < 13:05:18, 0.01 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step      1/625 (  0.2%) | Loss: 1.3719 | LR: 0.00e+00 | Time:    1.1min | Epoch: 0.00/1\n"
          ]
        }
      ],
      "source": [
        "# Create a custom callback for progress tracking\n",
        "class ProgressCallback(TrainerCallback):\n",
        "    \"\"\"Custom callback to track and display training progress\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.epoch_start_time = None\n",
        "        self.last_log_time = None\n",
        "        \n",
        "    def on_train_begin(self, args, state, control, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "        self.last_log_time = self.start_time\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TRAINING STARTED\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total epochs: {int(args.num_train_epochs)}\")\n",
        "        print(f\"Total steps: ~{state.max_steps:,}\")\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"Batch size: {args.per_device_train_batch_size}\")\n",
        "        print(\"=\" * 80 + \"\\n\")\n",
        "        \n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        self.epoch_start_time = time.time()\n",
        "        current_epoch = int(state.epoch) + 1\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EPOCH {current_epoch}/{int(args.num_train_epochs)}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            current_time = time.time()\n",
        "            elapsed_total = current_time - self.start_time\n",
        "            \n",
        "            # Display training progress\n",
        "            if 'loss' in logs and 'learning_rate' in logs:\n",
        "                step = state.global_step\n",
        "                loss = logs['loss']\n",
        "                lr = logs['learning_rate']\n",
        "                elapsed_since_last = current_time - self.last_log_time\n",
        "                \n",
        "                # Calculate progress percentage\n",
        "                progress_pct = (step / state.max_steps) * 100 if state.max_steps > 0 else 0\n",
        "                \n",
        "                print(f\"Step {step:6d}/{state.max_steps} ({progress_pct:5.1f}%) | \"\n",
        "                      f\"Loss: {loss:.4f} | LR: {lr:.2e} | \"\n",
        "                      f\"Time: {elapsed_total/60:6.1f}min | \"\n",
        "                      f\"Epoch: {state.epoch:.2f}/{args.num_train_epochs}\")\n",
        "                \n",
        "                self.last_log_time = current_time\n",
        "                \n",
        "            # Display evaluation results\n",
        "            if 'eval_loss' in logs:\n",
        "                print(f\"\\n{'─'*80}\")\n",
        "                print(f\"EVALUATION RESULTS (Step {state.global_step}):\")\n",
        "                print(f\"  Loss: {logs.get('eval_loss', 'N/A'):.4f}\")\n",
        "                print(f\"  Accuracy: {logs.get('eval_accuracy', 'N/A'):.4f}\")\n",
        "                print(f\"  F1-Macro: {logs.get('eval_f1_macro', 'N/A'):.4f}\")\n",
        "                eval_time = current_time - self.last_log_time\n",
        "                print(f\"  Eval time: {eval_time:.1f}s\")\n",
        "                print(f\"{'─'*80}\\n\")\n",
        "                self.last_log_time = current_time\n",
        "                \n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        epoch_time = time.time() - self.epoch_start_time\n",
        "        total_time = time.time() - self.start_time\n",
        "        current_epoch = int(state.epoch) + 1\n",
        "        \n",
        "        print(f\"\\n{'─'*80}\")\n",
        "        print(f\"Epoch {current_epoch} completed!\")\n",
        "        print(f\"  Epoch time: {epoch_time/60:.2f} minutes\")\n",
        "        print(f\"  Total time: {total_time/60:.2f} minutes\")\n",
        "        \n",
        "        if current_epoch < int(args.num_train_epochs):\n",
        "            remaining_epochs = int(args.num_train_epochs) - current_epoch\n",
        "            avg_epoch_time = total_time / current_epoch\n",
        "            estimated_remaining = (remaining_epochs * avg_epoch_time) / 60\n",
        "            print(f\"  Estimated time remaining: {estimated_remaining:.2f} minutes\")\n",
        "        print(f\"{'─'*80}\\n\")\n",
        "        \n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        total_time = time.time() - self.start_time\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TRAINING COMPLETED\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "        print(f\"Total steps completed: {state.global_step}\")\n",
        "        print(f\"Epochs completed: {state.epoch:.2f}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Ensure model is on CPU if USE_CPU is True\n",
        "if USE_CPU:\n",
        "    model = model.to('cpu')\n",
        "    device = torch.device('cpu')\n",
        "    print(\"Ensuring model is on CPU for training...\")\n",
        "    print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Create trainer with progress callback\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=test_tokenized,  # Using test set for evaluation during training\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(early_stopping_patience=2),  # Stop if no improvement for 2 epochs\n",
        "        ProgressCallback()  # Custom progress tracker\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Trainer created with progress tracking!\")\n",
        "print(\"Starting fine-tuning...\")\n",
        "print(\"Progress will be displayed below:\\n\")\n",
        "\n",
        "# Record training start time\n",
        "training_start_time = time.time()\n",
        "\n",
        "# Train the model (progress will be shown by the callback)\n",
        "trainer.train()\n",
        "\n",
        "# Record training end time\n",
        "training_time = time.time() - training_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"Fine-tuning completed!\")\n",
        "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Model on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set with progress tracking\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATING MODEL ON TEST SET\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Test samples: {len(test_tokenized):,}\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"Step 1: Running evaluation...\")\n",
        "eval_start = time.time()\n",
        "eval_results = trainer.evaluate()\n",
        "eval_time = time.time() - eval_start\n",
        "\n",
        "print(f\"\\n{'─'*80}\")\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"  Macro F1-Score: {eval_results['eval_f1_macro']:.4f}\")\n",
        "print(f\"  Evaluation time: {eval_time:.2f} seconds ({eval_time/60:.2f} minutes)\")\n",
        "print(f\"{'─'*80}\\n\")\n",
        "\n",
        "# Get predictions for detailed metrics\n",
        "print(\"Step 2: Generating predictions on full test set...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "pred_start = time.time()\n",
        "predictions = trainer.predict(test_tokenized)\n",
        "pred_time = time.time() - pred_start\n",
        "\n",
        "print(f\"Predictions completed in {pred_time:.2f} seconds ({pred_time/60:.2f} minutes)\")\n",
        "\n",
        "# Extract predictions and labels\n",
        "print(\"\\nStep 3: Processing predictions and calculating metrics...\")\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_test = predictions.label_ids\n",
        "y_pred_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "# Calculate additional metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"DETAILED METRICS\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Macro F1-Score: {f1_macro:.4f}\")\n",
        "print(f\"  Log Loss: {log_loss_score:.4f}\")\n",
        "print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Measure Inference Latency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Measure inference latency for 1000 samples with progress tracking\n",
        "print(\"=\" * 80)\n",
        "print(\"MEASURING INFERENCE LATENCY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Sample 1000 test examples\n",
        "num_samples = 1000\n",
        "print(f\"Sampling {num_samples} test examples...\")\n",
        "sample_indices = np.random.choice(len(test_tokenized), num_samples, replace=False)\n",
        "sample_data = test_tokenized.select(sample_indices)\n",
        "\n",
        "# Warm-up\n",
        "print(\"Warming up model (running 10 samples)...\")\n",
        "warmup_start = time.time()\n",
        "_ = trainer.predict(sample_data.select(range(10)))\n",
        "warmup_time = time.time() - warmup_start\n",
        "print(f\"Warm-up completed in {warmup_time:.2f} seconds\\n\")\n",
        "\n",
        "# Measure inference time\n",
        "print(f\"Measuring inference time for {num_samples} samples...\")\n",
        "print(\"This may take a minute or two...\")\n",
        "inference_start_time = time.time()\n",
        "_ = trainer.predict(sample_data)\n",
        "inference_time = time.time() - inference_start_time\n",
        "\n",
        "# Calculate latency per 1000 samples\n",
        "inference_latency_per_1k = inference_time\n",
        "\n",
        "print(f\"\\n{'─'*80}\")\n",
        "print(\"INFERENCE LATENCY RESULTS\")\n",
        "print(f\"{'─'*80}\")\n",
        "print(f\"Inference time for {num_samples} samples: {inference_time:.4f} seconds\")\n",
        "print(f\"Inference latency per 1,000 samples: {inference_latency_per_1k:.4f} seconds\")\n",
        "print(f\"Average latency per sample: {inference_latency_per_1k/1000:.4f} seconds\")\n",
        "print(f\"{'─'*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile results\n",
        "roberta_results = {\n",
        "    'Model': 'RoBERTa-base',\n",
        "    'Accuracy': accuracy,\n",
        "    'Macro F1-Score': f1_macro,\n",
        "    'Log Loss': log_loss_score,\n",
        "    'Training Time (s)': training_time,\n",
        "    'Inference Latency per 1k (s)': inference_latency_per_1k\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame([roberta_results])\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ROBERTA MODEL - RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ensure data directory exists\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('data/roberta_results.csv', index=False)\n",
        "print(\"\\nResults saved to data/roberta_results.csv\")\n",
        "\n",
        "# Save model with progress tracking\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAVING MODEL\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Saving model and tokenizer...\")\n",
        "print(\"This may take a minute...\")\n",
        "\n",
        "save_start = time.time()\n",
        "os.makedirs('data/roberta_ag_news_model', exist_ok=True)\n",
        "trainer.save_model('data/roberta_ag_news_model')\n",
        "print(\"  ✓ Model saved\")\n",
        "tokenizer.save_pretrained('data/roberta_ag_news_model')\n",
        "print(\"  ✓ Tokenizer saved\")\n",
        "save_time = time.time() - save_start\n",
        "\n",
        "print(f\"\\nModel saved to data/roberta_ag_news_model/\")\n",
        "print(f\"Save time: {save_time:.2f} seconds ({save_time/60:.2f} minutes)\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_labels, yticklabels=class_labels,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title('Confusion Matrix - RoBERTa-base', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/roberta_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Confusion matrix saved to data/roberta_confusion_matrix.png\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Compare with Classical Models\n",
        "\n",
        "Load classical models results and create comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load classical models results if available\n",
        "classical_results_path = 'data/classical_models_results.csv'\n",
        "\n",
        "if os.path.exists(classical_results_path):\n",
        "    classical_df = pd.read_csv(classical_results_path)\n",
        "    \n",
        "    # Combine results\n",
        "    all_results = pd.concat([classical_df, results_df], ignore_index=True)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"ALL MODELS - COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "    print(all_results.to_string(index=False))\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Save combined results\n",
        "    all_results.to_csv('data/all_models_results.csv', index=False)\n",
        "    print(\"\\nCombined results saved to data/all_models_results.csv\")\n",
        "    \n",
        "    # Visualization: F1-Score comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    models = all_results['Model'].values\n",
        "    f1_scores = all_results['Macro F1-Score'].values\n",
        "    training_times = all_results['Training Time (s)'].values\n",
        "    \n",
        "    # F1-Score comparison\n",
        "    axes[0].bar(models, f1_scores, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[0].set_title('Macro F1-Score Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Macro F1-Score', fontsize=12)\n",
        "    axes[0].set_ylim([0, 1])\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(f1_scores):\n",
        "        axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "    \n",
        "    # Training time comparison\n",
        "    axes[1].bar(models, training_times, color=['steelblue', 'coral', 'lightgreen', 'purple'])\n",
        "    axes[1].set_title('Training Time Comparison - All Models', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
        "    axes[1].set_yscale('log')  # Log scale for better visualization\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(training_times):\n",
        "        axes[1].text(i, v * 1.2, f'{v:.1f}s', ha='center', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('data/all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"Comparison visualization saved to data/all_models_comparison.png\")\n",
        "    \n",
        "else:\n",
        "    print(\"Classical models results not found. Run 02_Classical_Models.ipynb first.\")\n",
        "    print(\"RoBERTa results saved separately.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
