COE379L Project 3: Cross-Model Comparison for News Topic Classification
Advanced Classical Algorithms vs. Fine-Tuned Transformers

================================================================================
PROJECT OBJECTIVE
================================================================================
Perform a thorough, direct comparison on a standardized, multi-class text 
classification task between two different methodological paradigms:
1. Feature-engineered classical models (XGBoost, SVM with TF-IDF)
2. Fine-tuned deep learning models (RoBERTa transformers)

Goal: Determine the best modeling approach by balancing computational efficiency 
and predictive performance.

================================================================================
RESEARCH QUESTIONS
================================================================================
1. How significant is the performance gap between classical, feature-engineered 
   models and fine-tuned transformer models on a modern news classification task?
2. What are the trade-offs in training and inference time for each methodology?
3. Can a resource-efficient classical model provide sufficient performance to 
   justify avoiding the higher computational costs associated with fine-tuning a 
   large transformer?

================================================================================
DATASET: AG NEWS
================================================================================
Source: Hugging Face Datasets Hub (ag_news)

Description:
- 120,000+ training samples
- 7,600 test samples
- Pre-split into training and test sets
- Each sample includes news article title and short description (abstract)

Task/Classes (4 balanced categories):
- World
- Sports
- Business
- Sci/Tech

Data Preparation:
- Combine title and description fields into a single text input for 
  classification consistency across all models
- Use macro-averaged metrics due to balanced nature of dataset

================================================================================
DELIVERABLES
================================================================================

1. PROJECT CODE REPOSITORY (Jupyter Notebooks):
   a. Data Preprocessing and Exploratory Data Analysis (EDA)
   b. Classical Model Implementation and Optimization
      - XGBoost
      - SVM
      - TF-IDF feature extraction
   c. Transformer Model Fine-Tuning and Evaluation
      - RoBERTa fine-tuning

2. FINAL TECHNICAL REPORT (2 pages, excluding figures/appendices):
   a. Detailed Methodology
      - Feature engineering steps
      - Model selection rationale
      - Optimization procedures for all tested models
   
   b. Quantitative Results Table
      Performance Metrics:
      - Accuracy
      - Macro-Averaged F1-Score
      - Log Loss
      
      Efficiency Metrics:
      - Training time
      - Inference latency (per 1,000 samples)
   
   c. Discussion and Conclusion
      - Analysis of performance vs. efficiency trade-off
      - Recommendation for most suitable model given different resource 
        constraints

3. VISUALIZATIONS (Appendix/Figures):
   - Confusion Matrix for best-performing classical model
   - Confusion Matrix for best-performing transformer model
   - Bar Chart visualizing performance metrics (F1-score) of all finalized models

================================================================================
METHODOLOGY
================================================================================

CLASSICAL MODELS PIPELINE:
--------------------------
Feature Extraction:
- Term Frequency-Inverse Document Frequency (TF-IDF)
- Generate high-dimensional sparse vector representations
- Use unigrams and bigrams for entire corpus

Model 1: XGBoost (eXtreme Gradient Boosting)
- Use XGBClassifier integrated with scikit-learn pipelines
- Optimization: Search for optimal number of estimators, learning rate, 
  and tree depth via cross-validation

Model 2: Support Vector Machines (SVM)
- Implement LinearSVC
- Potentially use SVC with Radial Basis Function (RBF) kernel
- Use optimized TF-IDF feature space

Optimization:
- Grid Search/Randomized Search with Cross-Validation
- Used for robust hyperparameter tuning of both XGBoost and SVM models

TRANSFORMER (DEEP LEARNING) PIPELINE:
-------------------------------------
Base Model: RoBERTa-base
- Chosen over BERT for robust pre-training approach
- Dynamic masking, larger batch sizes
- No Next Sentence Prediction task
- Typically yields higher performance on downstream classification tasks

Training Method: Fine-Tuning
- Load pre-trained RoBERTa from Hugging Face Transformers library
- Add dense classification layer
- Train entire network end-to-end on AG News dataset

Technology Stack:
- PyTorch/TensorFlow and Hugging Face Ecosystem
- Efficient model loading, tokenization, training loop management
- GPU acceleration support

Tokenization/Embedding:
- Contextual Embeddings
- Input text processed using RoBERTa tokenizer
- Generate dynamic vector representations that capture word meaning based 
  on surrounding context

================================================================================
KEY METRICS TO REPORT
================================================================================
Performance Metrics:
- Accuracy
- Macro-Averaged F1-Score
- Log Loss

Efficiency Metrics:
- Training time
- Inference latency (per 1,000 samples)

================================================================================
NOTES
================================================================================
- Focus on balanced comparison between classical and transformer approaches
- Emphasize trade-offs between computational cost and predictive performance
- Provide practical recommendations for real-world news classification scenarios
- Use macro-averaged metrics due to balanced dataset nature

