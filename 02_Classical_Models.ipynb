{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COE379L Project 3: Classical Model Implementation and Optimization\n",
        "\n",
        "## Cross-Model Comparison for News Topic Classification\n",
        "\n",
        "This notebook covers:\n",
        "- TF-IDF feature extraction (unigrams and bigrams)\n",
        "- XGBoost classifier with hyperparameter optimization\n",
        "- Support Vector Machine (SVM) implementation (LinearSVC and RBF kernel)\n",
        "- Model evaluation and performance metrics\n",
        "- Training time and inference latency measurement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    f1_score, \n",
        "    log_loss, \n",
        "    confusion_matrix, \n",
        "    classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# XGBoost\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Set style for visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded preprocessed data from CSV files\n",
            "Training samples: 120,000\n",
            "Test samples: 7,600\n",
            "\n",
            "Class distribution (training):\n",
            "label\n",
            "0    30000\n",
            "1    30000\n",
            "2    30000\n",
            "3    30000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "X_train shape: (120000,)\n",
            "y_train shape: (120000,)\n",
            "X_test shape: (7600,)\n",
            "y_test shape: (7600,)\n"
          ]
        }
      ],
      "source": [
        "# Load preprocessed data from the EDA notebook\n",
        "# If data files don't exist, we'll load from Hugging Face and preprocess\n",
        "import os\n",
        "\n",
        "if os.path.exists('data/train_processed.csv') and os.path.exists('data/test_processed.csv'):\n",
        "    train_df = pd.read_csv('data/train_processed.csv')\n",
        "    test_df = pd.read_csv('data/test_processed.csv')\n",
        "    print(\"Loaded preprocessed data from CSV files\")\n",
        "else:\n",
        "    # Fallback: Load and preprocess from Hugging Face\n",
        "    from datasets import load_dataset\n",
        "    \n",
        "    print(\"Loading data from Hugging Face...\")\n",
        "    dataset = load_dataset(\"ag_news\")\n",
        "    train_df = pd.DataFrame(dataset['train'])\n",
        "    test_df = pd.DataFrame(dataset['test'])\n",
        "    \n",
        "    # Note: AG News dataset has a single 'text' field that already contains title and description\n",
        "    # The format is typically \"Title. Description\" - we'll use it directly as combined_text\n",
        "    train_df['combined_text'] = train_df['text'].astype(str).str.strip()\n",
        "    test_df['combined_text'] = test_df['text'].astype(str).str.strip()\n",
        "    \n",
        "    # Keep only necessary columns\n",
        "    train_df = train_df[['label', 'combined_text']]\n",
        "    test_df = test_df[['label', 'combined_text']]\n",
        "\n",
        "print(f\"Training samples: {len(train_df):,}\")\n",
        "print(f\"Test samples: {len(test_df):,}\")\n",
        "print(f\"\\nClass distribution (training):\")\n",
        "print(train_df['label'].value_counts().sort_index())\n",
        "\n",
        "# Prepare features and labels\n",
        "X_train = train_df['combined_text'].values\n",
        "y_train = train_df['label'].values\n",
        "X_test = test_df['combined_text'].values\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "print(f\"\\nX_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. TF-IDF Feature Extraction\n",
        "\n",
        "According to the project requirements, we need to generate high-dimensional sparse vector representations using unigrams and bigrams.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing TF-IDF vectorizer...\n",
            "Fitting TF-IDF on training data...\n",
            "Transforming test data...\n",
            "\n",
            "TF-IDF Feature Extraction Complete!\n",
            "Training TF-IDF fit time: 6.00 seconds\n",
            "Training features shape: (120000, 50000)\n",
            "Test features shape: (7600, 50000)\n",
            "Number of features: 50,000\n",
            "Sparsity: 99.95%\n"
          ]
        }
      ],
      "source": [
        "# Initialize TF-IDF vectorizer with unigrams and bigrams\n",
        "# Using max_features to limit dimensionality for computational efficiency\n",
        "# Adjust max_features based on available memory and computational resources\n",
        "print(\"Initializing TF-IDF vectorizer...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "    max_features=50000,  # Limit to top 50k features for efficiency\n",
        "    min_df=2,            # Ignore terms that appear in fewer than 2 documents\n",
        "    max_df=0.95,         # Ignore terms that appear in more than 95% of documents\n",
        "    sublinear_tf=True,   # Apply sublinear tf scaling (1 + log(tf))\n",
        "    stop_words='english' # Remove English stop words\n",
        ")\n",
        "\n",
        "# Fit and transform training data\n",
        "print(\"Fitting TF-IDF on training data...\")\n",
        "start_time = time.time()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_fit_time = time.time() - start_time\n",
        "\n",
        "# Transform test data\n",
        "print(\"Transforming test data...\")\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"\\nTF-IDF Feature Extraction Complete!\")\n",
        "print(f\"Training TF-IDF fit time: {tfidf_fit_time:.2f} seconds\")\n",
        "print(f\"Training features shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Test features shape: {X_test_tfidf.shape}\")\n",
        "print(f\"Number of features: {X_train_tfidf.shape[1]:,}\")\n",
        "print(f\"Sparsity: {(1.0 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Model Evaluation\n",
        "\n",
        "We'll create functions to evaluate models and measure training/inference times.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate a trained model and return metrics.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X_test: Test features\n",
        "        y_test: Test labels\n",
        "        model_name: Name of the model for display\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with metrics\n",
        "    \"\"\"\n",
        "    # Predictions\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    # Probabilities (for log loss)\n",
        "    try:\n",
        "        y_pred_proba = model.predict_proba(X_test)\n",
        "    except:\n",
        "        # Some models might not have predict_proba\n",
        "        y_pred_proba = None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "    \n",
        "    log_loss_score = None\n",
        "    if y_pred_proba is not None:\n",
        "        log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "    \n",
        "    # Inference latency per 1000 samples\n",
        "    num_samples = len(X_test)\n",
        "    inference_latency_per_1k = (inference_time / num_samples) * 1000\n",
        "    \n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'log_loss': log_loss_score,\n",
        "        'inference_time': inference_time,\n",
        "        'inference_latency_per_1k': inference_latency_per_1k,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "def measure_inference_latency(model, X_test, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Measure inference latency for a specific number of samples.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X_test: Test features\n",
        "        num_samples: Number of samples to test (default 1000)\n",
        "    \n",
        "    Returns:\n",
        "        Time taken for inference\n",
        "    \"\"\"\n",
        "    # Sample random indices\n",
        "    indices = np.random.choice(len(X_test), min(num_samples, len(X_test)), replace=False)\n",
        "    X_sample = X_test[indices]\n",
        "    \n",
        "    # Warm-up\n",
        "    _ = model.predict(X_sample[:10])\n",
        "    \n",
        "    # Measure inference time\n",
        "    start_time = time.time()\n",
        "    _ = model.predict(X_sample)\n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    return inference_time\n",
        "\n",
        "print(\"Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "XGBoost Model Training\n",
            "================================================================================\n",
            "\n",
            "Step 1: Hyperparameter tuning on subset of data...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "sparse array length is ambiguous; use getnnz() or shape[0]",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Use a subset for faster hyperparameter search\u001b[39;00m\n\u001b[32m     13\u001b[39m subset_size = \u001b[32m20000\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m indices = np.random.choice(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m)\u001b[49m, subset_size, replace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     15\u001b[39m X_train_subset = X_train_tfidf[indices]\n\u001b[32m     16\u001b[39m y_train_subset = y_train[indices]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Github/COE379L-Project3/venv/lib/python3.13/site-packages/scipy/sparse/_base.py:449\u001b[39m, in \u001b[36m_spbase.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse array length is ambiguous; use getnnz()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33m or shape[0]\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: sparse array length is ambiguous; use getnnz() or shape[0]"
          ]
        }
      ],
      "source": [
        "# Initialize XGBoost classifier\n",
        "# Note: XGBoost works better with dense matrices, but can handle sparse\n",
        "# For large sparse matrices, we might need to convert or use a subset\n",
        "print(\"=\" * 80)\n",
        "print(\"XGBoost Model Training\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# For computational efficiency with large sparse matrices, we'll use a subset for hyperparameter tuning\n",
        "# Then train final model on full dataset with best parameters\n",
        "print(\"\\nStep 1: Hyperparameter tuning on subset of data...\")\n",
        "\n",
        "# Use a subset for faster hyperparameter search\n",
        "subset_size = 20000\n",
        "indices = np.random.choice(len(X_train_tfidf), subset_size, replace=False)\n",
        "X_train_subset = X_train_tfidf[indices]\n",
        "y_train_subset = y_train[indices]\n",
        "\n",
        "# Convert sparse matrix to dense for XGBoost (or use sparse matrix support)\n",
        "# XGBoost supports sparse matrices, but dense might be faster for smaller subsets\n",
        "X_train_subset_dense = X_train_subset.toarray()\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize base model\n",
        "xgb_base = XGBClassifier(\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "# Use RandomizedSearchCV for faster search\n",
        "print(\"Running RandomizedSearchCV...\")\n",
        "start_time = time.time()\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb_base,\n",
        "    param_distributions=param_grid_xgb,\n",
        "    n_iter=10,  # Number of parameter settings sampled\n",
        "    cv=3,        # 3-fold cross-validation\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search_xgb.fit(X_train_subset_dense, y_train_subset)\n",
        "hyperparameter_search_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nHyperparameter search completed in {hyperparameter_search_time:.2f} seconds\")\n",
        "print(f\"Best parameters: {random_search_xgb.best_params_}\")\n",
        "print(f\"Best CV score: {random_search_xgb.best_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Train final XGBoost model on full dataset with best parameters\n",
        "print(\"\\nStep 2: Training final XGBoost model on full dataset...\")\n",
        "\n",
        "# For full dataset, we'll use a sample if the dataset is too large\n",
        "# XGBoost can handle large datasets, but for memory efficiency, we might sample\n",
        "full_train_size = min(50000, len(X_train_tfidf))  # Use up to 50k samples for final training\n",
        "if full_train_size < len(X_train_tfidf):\n",
        "    print(f\"Using {full_train_size:,} samples for final training (for computational efficiency)\")\n",
        "    indices_full = np.random.choice(len(X_train_tfidf), full_train_size, replace=False)\n",
        "    X_train_final = X_train_tfidf[indices_full].toarray()\n",
        "    y_train_final = y_train[indices_full]\n",
        "else:\n",
        "    X_train_final = X_train_tfidf.toarray()\n",
        "    y_train_final = y_train\n",
        "\n",
        "# Create final model with best parameters\n",
        "xgb_final = XGBClassifier(\n",
        "    **random_search_xgb.best_params_,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "# Train final model\n",
        "print(\"Training final model...\")\n",
        "start_time = time.time()\n",
        "xgb_final.fit(X_train_final, y_train_final)\n",
        "xgb_training_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training completed in {xgb_training_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating XGBoost model...\")\n",
        "X_test_dense = X_test_tfidf.toarray()\n",
        "xgb_results = evaluate_model(xgb_final, X_test_dense, y_test, \"XGBoost\")\n",
        "\n",
        "print(f\"\\nXGBoost Results:\")\n",
        "print(f\"  Accuracy: {xgb_results['accuracy']:.4f}\")\n",
        "print(f\"  Macro F1-Score: {xgb_results['f1_macro']:.4f}\")\n",
        "print(f\"  Log Loss: {xgb_results['log_loss']:.4f}\")\n",
        "print(f\"  Training Time: {xgb_training_time:.2f} seconds\")\n",
        "print(f\"  Inference Latency (per 1,000 samples): {xgb_results['inference_latency_per_1k']:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Support Vector Machine (SVM) - LinearSVC\n",
        "\n",
        "We'll implement LinearSVC which is more efficient than SVC for large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LinearSVC is more efficient for large sparse matrices\n",
        "print(\"=\" * 80)\n",
        "print(\"SVM LinearSVC Model Training\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define parameter grid for LinearSVC\n",
        "param_grid_svm_linear = {\n",
        "    'C': [0.1, 1.0, 10.0, 100.0],\n",
        "    'penalty': ['l2'],\n",
        "    'loss': ['squared_hinge'],\n",
        "    'max_iter': [1000, 2000]\n",
        "}\n",
        "\n",
        "# Initialize base model\n",
        "svm_linear_base = LinearSVC(random_state=42, dual=False)  # dual=False for n_samples > n_features\n",
        "\n",
        "# Use RandomizedSearchCV\n",
        "print(\"Running RandomizedSearchCV for LinearSVC...\")\n",
        "start_time = time.time()\n",
        "random_search_svm_linear = RandomizedSearchCV(\n",
        "    estimator=svm_linear_base,\n",
        "    param_distributions=param_grid_svm_linear,\n",
        "    n_iter=8,\n",
        "    cv=3,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Use subset for hyperparameter tuning\n",
        "random_search_svm_linear.fit(X_train_subset, y_train_subset)\n",
        "svm_linear_hyperparameter_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nHyperparameter search completed in {svm_linear_hyperparameter_time:.2f} seconds\")\n",
        "print(f\"Best parameters: {random_search_svm_linear.best_params_}\")\n",
        "print(f\"Best CV score: {random_search_svm_linear.best_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final LinearSVC model on full dataset\n",
        "print(\"\\nTraining final LinearSVC model on full dataset...\")\n",
        "\n",
        "svm_linear_final = LinearSVC(\n",
        "    **random_search_svm_linear.best_params_,\n",
        "    random_state=42,\n",
        "    dual=False\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "svm_linear_final.fit(X_train_tfidf, y_train)\n",
        "svm_linear_training_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training completed in {svm_linear_training_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating LinearSVC model...\")\n",
        "svm_linear_results = evaluate_model(svm_linear_final, X_test_tfidf, y_test, \"SVM-LinearSVC\")\n",
        "\n",
        "# LinearSVC doesn't have predict_proba by default, so log_loss will be None\n",
        "# This is expected behavior for LinearSVC\n",
        "print(f\"\\nSVM LinearSVC Results:\")\n",
        "print(f\"  Accuracy: {svm_linear_results['accuracy']:.4f}\")\n",
        "print(f\"  Macro F1-Score: {svm_linear_results['f1_macro']:.4f}\")\n",
        "print(f\"  Log Loss: {svm_linear_results['log_loss']} (LinearSVC doesn't support probability estimates)\")\n",
        "print(f\"  Training Time: {svm_linear_training_time:.2f} seconds\")\n",
        "print(f\"  Inference Latency (per 1,000 samples): {svm_linear_results['inference_latency_per_1k']:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Support Vector Machine (SVM) - RBF Kernel\n",
        "\n",
        "We'll also implement SVC with RBF kernel for comparison, though it's computationally more expensive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVC with RBF kernel - more computationally expensive\n",
        "# We'll use a smaller subset due to computational constraints\n",
        "print(\"=\" * 80)\n",
        "print(\"SVM RBF Kernel Model Training\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Note: RBF kernel is computationally expensive. Using smaller subset for training.\")\n",
        "\n",
        "# Use smaller subset for RBF kernel\n",
        "rbf_subset_size = 10000\n",
        "indices_rbf = np.random.choice(len(X_train_tfidf), rbf_subset_size, replace=False)\n",
        "X_train_rbf = X_train_tfidf[indices_rbf].toarray()  # RBF needs dense matrix\n",
        "y_train_rbf = y_train[indices_rbf]\n",
        "\n",
        "# Define parameter grid for RBF SVC\n",
        "param_grid_svm_rbf = {\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Initialize base model\n",
        "svm_rbf_base = SVC(kernel='rbf', random_state=42, probability=True)  # probability=True for predict_proba\n",
        "\n",
        "# Use RandomizedSearchCV\n",
        "print(\"Running RandomizedSearchCV for RBF SVC...\")\n",
        "start_time = time.time()\n",
        "random_search_svm_rbf = RandomizedSearchCV(\n",
        "    estimator=svm_rbf_base,\n",
        "    param_distributions=param_grid_svm_rbf,\n",
        "    n_iter=6,  # Fewer iterations due to computational cost\n",
        "    cv=3,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search_svm_rbf.fit(X_train_rbf, y_train_rbf)\n",
        "svm_rbf_hyperparameter_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nHyperparameter search completed in {svm_rbf_hyperparameter_time:.2f} seconds\")\n",
        "print(f\"Best parameters: {random_search_svm_rbf.best_params_}\")\n",
        "print(f\"Best CV score: {random_search_svm_rbf.best_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final RBF SVC model\n",
        "print(\"\\nTraining final RBF SVC model...\")\n",
        "\n",
        "svm_rbf_final = SVC(\n",
        "    **random_search_svm_rbf.best_params_,\n",
        "    kernel='rbf',\n",
        "    random_state=42,\n",
        "    probability=True\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "svm_rbf_final.fit(X_train_rbf, y_train_rbf)\n",
        "svm_rbf_training_time = time.time() - start_time\n",
        "\n",
        "print(f\"Training completed in {svm_rbf_training_time:.2f} seconds\")\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating RBF SVC model...\")\n",
        "X_test_dense = X_test_tfidf.toarray()\n",
        "svm_rbf_results = evaluate_model(svm_rbf_final, X_test_dense, y_test, \"SVM-RBF\")\n",
        "\n",
        "print(f\"\\nSVM RBF Results:\")\n",
        "print(f\"  Accuracy: {svm_rbf_results['accuracy']:.4f}\")\n",
        "print(f\"  Macro F1-Score: {svm_rbf_results['f1_macro']:.4f}\")\n",
        "print(f\"  Log Loss: {svm_rbf_results['log_loss']:.4f}\")\n",
        "print(f\"  Training Time: {svm_rbf_training_time:.2f} seconds\")\n",
        "print(f\"  Inference Latency (per 1,000 samples): {svm_rbf_results['inference_latency_per_1k']:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Summary and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile all results\n",
        "results_summary = []\n",
        "\n",
        "# Add XGBoost results\n",
        "results_summary.append({\n",
        "    'Model': 'XGBoost',\n",
        "    'Accuracy': xgb_results['accuracy'],\n",
        "    'Macro F1-Score': xgb_results['f1_macro'],\n",
        "    'Log Loss': xgb_results['log_loss'],\n",
        "    'Training Time (s)': xgb_training_time,\n",
        "    'Inference Latency per 1k (s)': xgb_results['inference_latency_per_1k']\n",
        "})\n",
        "\n",
        "# Add LinearSVC results\n",
        "results_summary.append({\n",
        "    'Model': 'SVM-LinearSVC',\n",
        "    'Accuracy': svm_linear_results['accuracy'],\n",
        "    'Macro F1-Score': svm_linear_results['f1_macro'],\n",
        "    'Log Loss': svm_linear_results['log_loss'] if svm_linear_results['log_loss'] is not None else np.nan,\n",
        "    'Training Time (s)': svm_linear_training_time,\n",
        "    'Inference Latency per 1k (s)': svm_linear_results['inference_latency_per_1k']\n",
        "})\n",
        "\n",
        "# Add RBF SVC results\n",
        "results_summary.append({\n",
        "    'Model': 'SVM-RBF',\n",
        "    'Accuracy': svm_rbf_results['accuracy'],\n",
        "    'Macro F1-Score': svm_rbf_results['f1_macro'],\n",
        "    'Log Loss': svm_rbf_results['log_loss'],\n",
        "    'Training Time (s)': svm_rbf_training_time,\n",
        "    'Inference Latency per 1k (s)': svm_rbf_results['inference_latency_per_1k']\n",
        "})\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame(results_summary)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLASSICAL MODELS - RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('data/classical_models_results.csv', index=False)\n",
        "print(\"\\nResults saved to data/classical_models_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart comparing F1-scores\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# F1-Score comparison\n",
        "models = results_df['Model'].values\n",
        "f1_scores = results_df['Macro F1-Score'].values\n",
        "\n",
        "axes[0].bar(models, f1_scores, color=['steelblue', 'coral', 'lightgreen'])\n",
        "axes[0].set_title('Macro F1-Score Comparison - Classical Models', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Macro F1-Score', fontsize=12)\n",
        "axes[0].set_ylim([0, 1])\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(f1_scores):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Training time comparison\n",
        "training_times = results_df['Training Time (s)'].values\n",
        "axes[1].bar(models, training_times, color=['steelblue', 'coral', 'lightgreen'])\n",
        "axes[1].set_title('Training Time Comparison - Classical Models', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
        "axes[1].set_yscale('log')  # Log scale for better visualization\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(training_times):\n",
        "    axes[1].text(i, v * 1.2, f'{v:.1f}s', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/classical_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Visualization saved to data/classical_models_comparison.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Confusion Matrices\n",
        "\n",
        "We'll create confusion matrices for the best-performing classical model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model based on F1-score\n",
        "best_model_idx = results_df['Macro F1-Score'].idxmax()\n",
        "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
        "\n",
        "print(f\"Best performing classical model: {best_model_name}\")\n",
        "print(f\"F1-Score: {results_df.loc[best_model_idx, 'Macro F1-Score']:.4f}\")\n",
        "\n",
        "# Get predictions for best model\n",
        "if best_model_name == 'XGBoost':\n",
        "    best_predictions = xgb_results['y_pred']\n",
        "    best_model_obj = xgb_final\n",
        "elif best_model_name == 'SVM-LinearSVC':\n",
        "    best_predictions = svm_linear_results['y_pred']\n",
        "    best_model_obj = svm_linear_final\n",
        "else:  # SVM-RBF\n",
        "    best_predictions = svm_rbf_results['y_pred']\n",
        "    best_model_obj = svm_rbf_final\n",
        "\n",
        "# Class labels\n",
        "class_labels = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=class_labels, yticklabels=class_labels,\n",
        "            cbar_kws={'label': 'Count'})\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/classical_models_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"Confusion matrix saved to data/classical_models_confusion_matrix.png\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, best_predictions, target_names=class_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Models and Results\n",
        "\n",
        "We'll save the trained models and all results for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models using joblib\n",
        "import joblib\n",
        "\n",
        "print(\"Saving models and vectorizer...\")\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "joblib.dump(tfidf_vectorizer, 'data/tfidf_vectorizer.pkl')\n",
        "print(\"✓ TF-IDF vectorizer saved\")\n",
        "\n",
        "# Save XGBoost model\n",
        "joblib.dump(xgb_final, 'data/xgb_model.pkl')\n",
        "print(\"✓ XGBoost model saved\")\n",
        "\n",
        "# Save LinearSVC model\n",
        "joblib.dump(svm_linear_final, 'data/svm_linear_model.pkl')\n",
        "print(\"✓ SVM LinearSVC model saved\")\n",
        "\n",
        "# Save RBF SVC model\n",
        "joblib.dump(svm_rbf_final, 'data/svm_rbf_model.pkl')\n",
        "print(\"✓ SVM RBF model saved\")\n",
        "\n",
        "print(\"\\nAll models and results saved successfully!\")\n",
        "print(\"Ready for comparison with transformer models in the next notebook.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
